{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3c4afc",
   "metadata": {},
   "source": [
    "# Project: Winning Jeopardy\n",
    "\n",
    "The following dataset contains 20,000 rows of the full dataset of jeopardy questions. The goal of the jeopardy analysis project is to find out if there are certain strategies that could help a person win based on the data and see any other patterns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f5b13c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
      "       ' Question', ' Answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "jeopardy= pd.read_csv('data/jeopardy.csv')\n",
    "print(jeopardy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17f88e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71857855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Show Number', 'Air Date', 'Round', 'Category', 'Value', 'Question',\n",
      "       'Answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "columns = []\n",
    "for col in jeopardy.columns:\n",
    "    col = col.strip()\n",
    "    columns.append(col)\n",
    "jeopardy.columns = columns\n",
    "print(jeopardy.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01693f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#The function normale cleans the question column so the text is normalised for analysis\n",
    "def normale(stri):\n",
    "    stri = stri.lower()\n",
    "    for char in string.punctuation:\n",
    "        stri = stri.replace(char, '')\n",
    "    return stri\n",
    "jeopardy['clean_question'] = jeopardy['Question'].apply(normale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6761fbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    for the last 8 years of his life galileo was u...\n",
      "1    no 2 1912 olympian football star at carlisle i...\n",
      "2    the city of yuma in this state has a record av...\n",
      "3    in 1963 live on the art linkletter show this c...\n",
      "4    signer of the dec of indep framer of the const...\n",
      "Name: clean_question, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(jeopardy['clean_question'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "138fe99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    copernicus\n",
      "1    jim thorpe\n",
      "2       arizona\n",
      "3     mcdonalds\n",
      "4    john adams\n",
      "Name: clean_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Uses same function to normalise answers\n",
    "jeopardy['clean_answer'] = jeopardy['Answer'].apply(normale)\n",
    "print(jeopardy['clean_answer'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b821d32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    200\n",
      "1    200\n",
      "2    200\n",
      "3    200\n",
      "4    200\n",
      "5    200\n",
      "6    400\n",
      "7    400\n",
      "8    400\n",
      "9    400\n",
      "Name: clean_value, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#For dollar values, takes out commas and dollar signs and converts them to integers\n",
    "def normal_dollar(stri):\n",
    "    stri = str(stri)\n",
    "    stri = stri.replace(',', '')\n",
    "    stri = stri.replace('$', '')\n",
    "    if stri == 'nan' or stri == 'None':\n",
    "        return 0\n",
    "    else:\n",
    "        stri = int(stri)\n",
    "        return stri\n",
    "jeopardy['clean_value'] = jeopardy['Value'].apply(normal_dollar)\n",
    "\n",
    "print(jeopardy['clean_value'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91b76e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['Air Date'] = pd.to_datetime(jeopardy['Air Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "17060a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This checks for each row of the data, whether the answer is found inside the question. \n",
    "def detect_matches(row):\n",
    "    split_answer = row['clean_answer'].split()\n",
    "    split_question = row['clean_question'].split()\n",
    "    if \"the\" in split_answer:\n",
    "        split_answer.remove(\"the\")\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    match_count  = 0 \n",
    "    for word in split_answer:\n",
    "        if word in split_question:\n",
    "            match_count+=1\n",
    "        return match_count/len(split_answer)\n",
    "jeopardy['answer_in_question'] = jeopardy.apply(detect_matches, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91331d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Show Number   Air Date             Round           Category  Value  \\\n",
      "31          4680 2004-12-31  Double Jeopardy!     AIRLINE TRAVEL   $400   \n",
      "67          5957 2010-07-06         Jeopardy!  RHYMES WITH SMART   $400   \n",
      "73          5957 2010-07-06         Jeopardy!  RHYMES WITH SMART   $600   \n",
      "79          5957 2010-07-06         Jeopardy!  RHYMES WITH SMART   $800   \n",
      "100         5957 2010-07-06  Double Jeopardy!     JUST THE FACTS  $1200   \n",
      "\n",
      "                                              Question             Answer  \\\n",
      "31   It can be a place to leave your puppy when you...           a kennel   \n",
      "67   Small, slender missile thrown at a board in a ...             a dart   \n",
      "73   It can be a separating line in your hair or a ...             a part   \n",
      "79             A graphic representation of information            a chart   \n",
      "100  <a href=\"http://www.j-archive.com/media/2010-0...  a German Shepherd   \n",
      "\n",
      "                                        clean_question       clean_answer  \\\n",
      "31   it can be a place to leave your puppy when you...           a kennel   \n",
      "67   small slender missile thrown at a board in a game             a dart   \n",
      "73   it can be a separating line in your hair or a ...             a part   \n",
      "79             a graphic representation of information            a chart   \n",
      "100  a hrefhttpwwwjarchivecommedia20100706dj14jpg t...  a german shepherd   \n",
      "\n",
      "     clean_value  answer_in_question  \n",
      "31           400            0.500000  \n",
      "67           400            0.500000  \n",
      "73           600            0.500000  \n",
      "79           800            0.500000  \n",
      "100         1200            0.333333  \n"
     ]
    }
   ],
   "source": [
    "#Sees if after the filtering there is still data\n",
    "print(jeopardy[jeopardy['answer_in_question']!=0].head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6170143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029624080410369708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19325   1984-09-21\n",
       "19301   1984-09-21\n",
       "19302   1984-09-21\n",
       "19303   1984-09-21\n",
       "19304   1984-09-21\n",
       "           ...    \n",
       "1953    2012-01-19\n",
       "1954    2012-01-19\n",
       "1955    2012-01-19\n",
       "1945    2012-01-19\n",
       "1922    2012-01-19\n",
       "Name: Air Date, Length: 19999, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(jeopardy['answer_in_question'].mean()) \n",
    "jeopardy['Air Date'].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf23982",
   "metadata": {},
   "source": [
    "Around three percent of answers are inside the corresponding question. With such a low chance, it is a bad idea assume the answer will be in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3714b54d",
   "metadata": {},
   "source": [
    "This compiles a set of all unique terms from the questions and also keeps track of a match count, which is seeing \n",
    "if a new word is already in the set of terms. The question overlap list takes for each row, what proportion of terms in the question are previously used terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df883c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_used = set()\n",
    "question_overlap = []\n",
    "for index, row in jeopardy.iterrows():\n",
    "    split_question = row['clean_question'].split()\n",
    "    split_question = [word for word in split_question if len(word)>5]\n",
    "    match_count = 0\n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count+= 1\n",
    "        terms_used.add(word)\n",
    "    if len(split_question)>0:\n",
    "        question_overlap.append(match_count/len(split_question))\n",
    "    else: \n",
    "        question_overlap.append(0)\n",
    "jeopardy['question_overlap'] = question_overlap \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1407e19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6919577992203563\n"
     ]
    }
   ],
   "source": [
    "print(jeopardy['question_overlap'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a8f8a",
   "metadata": {},
   "source": [
    "This number is larger than I expected! Terms used in the questions here are lots of times found in previous questions. Based on term overlap, a good strategy is to study other important words in each question than the answer because these words and topics are likely to appear in future questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c6326c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3),\n",
       " (1, 5),\n",
       " (0, 1),\n",
       " (6, 3),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import choice\n",
    "def high(row):\n",
    "    if row['clean_value']>800:\n",
    "        value = 1\n",
    "    else: \n",
    "        value = 0\n",
    "    return value\n",
    "jeopardy['high_value'] = jeopardy.apply(high, axis=1)\n",
    "#For each word count the number of times word is in high value vs low value question\n",
    "def high_low(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    for index, row in jeopardy.iterrows():\n",
    "        split_question = row['clean_question'].split(\" \")\n",
    "        if word in split_question:\n",
    "            if row['high_value'] == 1:\n",
    "                high_count+=1\n",
    "            else:\n",
    "                low_count+=1\n",
    "    return high_count, low_count\n",
    "\n",
    "terms_used_list =list(terms_used)\n",
    "comparison_terms = [choice(terms_used_list) for _ in range(10)]\n",
    "observed_expected = [] #High-low value pairs for each term\n",
    "for term in comparison_terms:\n",
    "    observed_expected.append(high_low(term))\n",
    "    \n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44512ec3",
   "metadata": {},
   "source": [
    "Summary of the above function:\n",
    "\n",
    "1)Makes a new column in jeopardy that sets a question to high value if the value is >800 dollars\n",
    "\n",
    "2)Creates a function that counts how many times a given word is in either a high value or low value question, returns those values in a list\n",
    "\n",
    "3)Draws a sample of 10 words, and appends the high_low count for each word in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7946b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_count = jeopardy[jeopardy['high_value'] == 1].shape[0]#shape[0] denotes length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f34894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_value_count = jeopardy[jeopardy['high_value'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e0a9316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chi_squared = []\n",
    "from scipy.stats import chisquare\n",
    "import numpy as np\n",
    "for obs_list in observed_expected:\n",
    "    total = sum(obs_list)\n",
    "    total_prop = total/jeopardy.shape[0]\n",
    "    high_prop = total_prop*high_value_count #expected high values and low values of the same sample size\n",
    "    low_prop = total_prop*low_value_count\n",
    "    \n",
    "    chisquared, pvalue = chisquare([obs_list[0], obs_list[1]], [high_prop, low_prop])\n",
    "    chi_squared.append([chisquared, pvalue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13f6b731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3137668167849311, 0.5753778622944691],\n",
       " [0.42281054506129573, 0.515537958129453],\n",
       " [0.401962846126884, 0.5260772985705469],\n",
       " [6.353131314909584, 0.011717429728771048],\n",
       " [2.487792117195675, 0.11473257634454047],\n",
       " [0.401962846126884, 0.5260772985705469],\n",
       " [2.487792117195675, 0.11473257634454047],\n",
       " [0.401962846126884, 0.5260772985705469],\n",
       " [0.401962846126884, 0.5260772985705469],\n",
       " [0.401962846126884, 0.5260772985705469]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda7cd0",
   "metadata": {},
   "source": [
    "The chisquare test was performed for each of the 10 terms from the sample list. It was comparing the proportion of high and low value questions for the term with the total proportion of high and low value terms for the whole dataset to see if each terms has a significant difference from the total proportion. \n",
    "\n",
    "The results show that only one of the terms have a p value less than the standard 0.05, which means that high and low value questions are not associated with specific terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8b9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fae0aafc",
   "metadata": {},
   "source": [
    "I would like to do a chisquare test to see if the expected length of question for high or value questions is the same as the expected length of general questions\n",
    "\n",
    "Steps\n",
    "\n",
    "1) Compute the average value of high value question length and low value question length\n",
    "\n",
    "2) Do chi2 for comparing high and low to expected value of a general question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02beb7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22    1939 Oscar winner: \"...you are a credit to you...\n",
      "24    This Asian political party was founded in 1885...\n",
      "25    No. 5: Only center to lead the NBA in assists;...\n",
      "26    The Kirschner brothers, Don & Bill, named this...\n",
      "27    Revolutionary War hero: \"His spirit is in Verm...\n",
      "Name: Question, dtype: object\n",
      "86.2193609680484\n",
      "106\n",
      "102\n",
      "105\n",
      "111\n",
      "83\n",
      "66\n",
      "104\n",
      "54\n",
      "33\n",
      "72\n",
      "33\n",
      "99\n",
      "56\n",
      "66\n",
      "114\n",
      "76\n",
      "86\n",
      "58\n",
      "89\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "high_value_list  = jeopardy[jeopardy['high_value'] == 1]\n",
    "low_value_list = jeopardy[jeopardy['high_value'] == 0]\n",
    "high_value_sample = [choice(list(high_value_list['Question'])) for _ in range(10)]\n",
    "low_value_sample = [choice(list(low_value_list['Question'])) for _ in range(10)]\n",
    "\n",
    "\n",
    "chi_low = []\n",
    "question_length_average = np.mean(jeopardy['Question'].str.len())\n",
    "print(question_length_average)\n",
    "def chi_question(sample):\n",
    "    chi_high = []\n",
    "    for question in sample:\n",
    "      \n",
    "        chisquared, pvalue = chisquare([len(question),question_length_average])\n",
    "        chi_high.append([chisquared, pvalue])\n",
    "    return chi_high\n",
    "\n",
    "def chi_question(sample):\n",
    "    \n",
    "    chi_low= []\n",
    "    for question in sample:\n",
    "        print(len(question))\n",
    "        \n",
    "        chisquared, pvalue = chisquare([len(question),question_length_average])\n",
    "        chi_low.append([chisquared, pvalue])  \n",
    "        \n",
    "    return chi_low\n",
    "\n",
    "high_list = chi_question(high_value_sample)\n",
    "low_list = chi_question(low_value_sample)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0d8cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi square results of high value question length samples\n",
      "\n",
      "[[2.0355581172564934, 0.15365777284612617], [1.3230762604651947, 0.25004070352402297], [1.8445433593275096, 0.1744193700071655], [3.1136906022698763, 0.07763685620590016], [0.061247631378010745, 0.8045347652108468], [2.685746118997656, 0.10124979119591554], [1.66203441529627, 0.1973288649260355], [7.40330874440334, 0.006510402096936676], [23.757050523081645, 1.0929427320661183e-06], [1.277910775916291, 0.25828785282901556]]\n",
      "\n",
      "\n",
      "Chi square results of low value question length samples\n",
      "\n",
      "____________\n",
      "[[23.757050523081645, 1.0929427320661183e-06], [0.8818988102071178, 0.34768213757831434], [6.421135428406073, 0.011277003800947514], [2.685746118997656, 0.10124979119591554], [3.8545917901853293, 0.04960999757522686], [0.6437908395893718, 0.422341403684643], [0.0002794066476187961, 0.986663612523435], [5.5216742613458125, 0.0187822743620027], [0.04412727784929335, 0.8336169909353146], [4.701531213250492, 0.03013575757337761]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Chi square results of high value question length samples\\n\")\n",
    "print(high_list)\n",
    "print(\"\\n\")\n",
    "print(\"Chi square results of low value question length samples\\n\")\n",
    "print(\"____________\")\n",
    "print(low_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424072c",
   "metadata": {},
   "source": [
    "Majority of p values in both the high value and low value question lengths is more than 0.05. The high p values of question lengths in either the high or low value length set are not significantly different from the expected value of the question length. So, the question length is not very different between high and low value questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9caa734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TELEVISION          51\n",
      "U.S. GEOGRAPHY      50\n",
      "LITERATURE          45\n",
      "BEFORE & AFTER      40\n",
      "AMERICAN HISTORY    40\n",
      "HISTORY             40\n",
      "AUTHORS             39\n",
      "WORD ORIGINS        38\n",
      "WORLD CAPITALS      37\n",
      "BODIES OF WATER     36\n",
      "SPORTS              36\n",
      "SCIENCE             35\n",
      "MAGAZINES           35\n",
      "RHYME TIME          35\n",
      "SCIENCE & NATURE    35\n",
      "WORLD GEOGRAPHY     33\n",
      "WORLD HISTORY       32\n",
      "HISTORIC NAMES      32\n",
      "ANNUAL EVENTS       32\n",
      "BIRDS               31\n",
      "Name: Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(jeopardy['Category'].value_counts()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f700f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_count(datalist):\n",
    "    terms_used = set()\n",
    "    question_overlap = []\n",
    "    for index, row in datalist.iterrows():\n",
    "        split_question = row['clean_question'].split()\n",
    "        split_question = [word for word in split_question if len(word)>5]\n",
    "        match_count = 0\n",
    "        for word in split_question:\n",
    "            if word in terms_used:\n",
    "                match_count+= 1\n",
    "            terms_used.add(word)\n",
    "        if len(split_question)>0:\n",
    "            question_overlap.append(match_count/len(split_question))\n",
    "        else: \n",
    "            question_overlap.append(0)\n",
    "    return question_overlap\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ec69724",
   "metadata": {},
   "outputs": [],
   "source": [
    "tele_jeopardy = jeopardy[jeopardy['Category']=='TELEVISION']\n",
    "geo_jeopardy = jeopardy[jeopardy['Category']=='U.S. GEOGRAPHY']\n",
    "sports_jeopardy = jeopardy[jeopardy['Category']=='SPORTS']\n",
    "american_jeopardy = jeopardy[jeopardy['Category'] == 'AMERICAN HISTORY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c1bc2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat terms for categores:\n",
      "\n",
      "Geography: \n",
      "0.22261904761904763\n",
      "\n",
      "\n",
      "Television: \n",
      "0.2222222222222222\n",
      "\n",
      "\n",
      "Sports: \n",
      "0.20634920634920637\n",
      "\n",
      "\n",
      "American: \n",
      "0.1309126984126984\n"
     ]
    }
   ],
   "source": [
    "print(\"Repeat terms for categores:\\n\")\n",
    "\n",
    "print(\"Geography: \")\n",
    "print(np.mean(match_count(geo_jeopardy)))\n",
    "print(\"\\n\")\n",
    "print(\"Television: \")\n",
    "print(np.mean(match_count(tele_jeopardy)))\n",
    "print(\"\\n\")\n",
    "print(\"Sports: \")\n",
    "print(np.mean(match_count(sports_jeopardy)))\n",
    "print(\"\\n\")\n",
    "print(\"American: \")\n",
    "print(np.mean(match_count(american_jeopardy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb627b",
   "metadata": {},
   "source": [
    "This final test does the repeat terms test that was done earlier in the analysis (seeing if terms in the questions were used in previous questions). Surprisingly, the per-category term use is far lower than the general term use decimal (0.69). This test is also useful to see which categories have similar topics. American History has least similar terms compared to geogrpahy, television and sports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a58254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1e779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
